{"cells":[{"cell_type":"markdown","metadata":{"id":"XoDj4i_ti-q8"},"source":["# Amazon Product Review Scraper"]},{"cell_type":"markdown","metadata":{"id":"rBf0F0aXjIox"},"source":["## Introduction to Web Scraping\n","\n","In the process of Data Analysis, we often need to retrieve data from the web, especially when a defined dataset is not available. This process is called **web scraping**. Web scraping involves automating the process of gathering information from web pages using programming languages like Python, along with libraries such as BeautifulSoup.\n","\n","In this notebook, we will scrape product reviews from Amazon, focusing on ethical and responsible data collection practices.\n"]},{"cell_type":"markdown","metadata":{"id":"88BQcIZnjNJb"},"source":["## Ethical Data Collection\n","\n","- **Respect Privacy**: Do not collect personal information without consent.\n","- **Follow Legal Guidelines**: Comply with local and international laws.\n","- **Respect Terms of Service**: Abide by the terms of service of the websites.\n","- **Ensure Data Security**: Protect the data from unauthorized access.\n","- **Minimize Data Collection**: Collect only the necessary data."]},{"cell_type":"markdown","metadata":{"id":"hI1fMSSUjRTE"},"source":["## Amazon Reviews Scraping\n","\n","In this notebook, we will:\n","1. Scrape product reviews from Amazon.\n","2. Save the reviews in a CSV file for further analysis.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"tEvDqB007uvo"},"source":["## Step-by-Step Guide\n","\n","Let's begin by importing the necessary libraries."]},{"cell_type":"markdown","metadata":{"id":"ax5s1j2J9LHO"},"source":["### Importing Libraries\n","\n","To scrape data from websites, we need to use some powerful Python libraries. Here are the ones we'll be using:\n","* `requests`: This library allows us to send HTTP requests to websites and get their HTML content. It's like our way of asking a website, \"Can I see your content, please?\"\n","* `BeautifulSoup`: This library helps us parse and navigate through the HTML content we get from websites. It's like a pair of super-powered glasses that let us see and extract the data we need from a webpage. Read more [here](https://beautiful-soup-4.readthedocs.io/en/latest/).\n","* `pandas`: This library is essential for data manipulation and analysis. It allows us to work with our scraped data in a structured format, like tables.\n","* `datetime`: This library helps us handle date and time data, which is often useful when working with reviews that have timestamps.\n","\n","Let's start by importing these libraries:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jjsYcJe_jSrs"},"outputs":[],"source":["# Import packages\n","import requests\n","import pandas as pd\n","from bs4 import BeautifulSoup\n","from datetime import datetime"]},{"cell_type":"markdown","metadata":{"id":"u_jlUCAkjVo-"},"source":["### Setting Up Headers\n","\n","When we make requests to a website, it’s important to mimic a real user to avoid getting blocked or flagged as a bot. This is where setting up headers comes into play. Headers contain information about the request being sent, such as the browser type, accepted response formats, and more. By including headers, we make our request look like it’s coming from a regular web browser.\n","\n","Here’s how we set up the headers:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S0tWXas9jYVI"},"outputs":[],"source":["# Header to set the requests as a browser requests\n","headers = {\n","    'authority': 'www.amazon.com',\n","    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',\n","    'accept-language': 'en-US,en;q=0.9,bn;q=0.8',\n","    'sec-ch-ua': '\" Not A;Brand\";v=\"99\", \"Chromium\";v=\"102\", \"Google Chrome\";v=\"102\"',\n","    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/102.0.0.0 Safari/537.36',\n","}"]},{"cell_type":"markdown","metadata":{"id":"9Fe1lquh98zZ"},"source":["Explanation:\n","* `authority`: Specifies the domain of the website.\n","* `accept`: Indicates the types of content the client can process.\n","* `accept-language`: Specifies the preferred languages for the response.\n","* `sec-ch-ua`: Provides information about the user agent (browser).\n","* `user-agent`: Identifies the browser and operating system being used. This is crucial for mimicking a real user.\n","\n","By setting up these headers, we make our HTTP requests appear as if they are coming from a regular user browsing the website. This helps in reducing the chances of our requests being blocked by the website's security measures."]},{"cell_type":"markdown","metadata":{"id":"82aSggjSmIDp"},"source":["### Input the Amazon URL\n","\n","Now that we've set up our headers, the next step is to get the URL of the Amazon product whose reviews we want to scrape. We'll ask the user to input this URL. This makes our script flexible, allowing us to scrape reviews for any product by simply providing its URL.\n","\n","Here's how we do it:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3691,"status":"ok","timestamp":1720017256980,"user":{"displayName":"Clément Sampebgo","userId":"18280352312023246054"},"user_tz":0},"id":"Z4QBFW3HmIuj","outputId":"6878cf0b-8504-4679-ce8b-7106bf3e77d7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Enter Amazon URL to retrieve reviews: https://www.amazon.com/JBL-130NC-Wireless-Cancelling-Headphones/dp/B09JB8KPNW\n","URL set to: https://www.amazon.com/JBL-130NC-Wireless-Cancelling-Headphones/dp/B09JB8KPNW\n"]}],"source":["# URL of the Amazon Review page\n","user_input = input(\"Enter Amazon URL to retrieve reviews: \")\n","reviews_url = user_input\n","\n","print(f\"URL set to: {reviews_url}\")"]},{"cell_type":"markdown","metadata":{"id":"zvoC9tJMmRLR"},"source":["## Defining Functions\n"]},{"cell_type":"markdown","metadata":{"id":"wIPWgdGSmT_F"},"source":["### Function to Extract HTML Data\n","\n","Now that we have the product URL, we need a function to extract the HTML data from the Amazon review pages. This function will handle sending requests to the Amazon server and parsing the HTML content using BeautifulSoup.\n","\n","Here's the function to extract HTML data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sX85pPN1mVsl"},"outputs":[],"source":["# Extract Data as Html object from amazon Review page\n","# This function will also extract the product title.\n","def reviewsHtml(url, len_page):\n","\n","    # Empty List define to store all pages html data\n","    soups = []\n","\n","\n","    # Loop for gather all reviews from all pages via range\n","    for page_no in range(1, len_page + 1):\n","\n","        # parameter set as page no to the requests body\n","        params = {\n","            'ie': 'UTF8',\n","            'reviewerType': 'all_reviews',\n","            'filterByStar': 'critical',\n","            'pageNumber': page_no,\n","        }\n","\n","        # Request make for each page\n","\n","        response = requests.get(url, headers=headers)\n","\n","        # Save Html object by using BeautifulSoup4 and lxml parser\n","        soup = BeautifulSoup(response.text, 'lxml')\n","\n","        if page_no == 1:\n","            title = soup.find(\"span\", attrs={\"id\":'productTitle'})\n","            title_str = title.string\n","            title_str = title_str.strip()\n","\n","        # Add single Html page data in master soups list\n","        soups.append(soup)\n","\n","    return soups,title_str"]},{"cell_type":"markdown","metadata":{"id":"IRoqPvvCmZPp"},"source":["### Function to Extract Review Data\n","\n","After collecting the HTML content, the next step is to extract the relevant review data from these pages. This involves parsing the HTML to find specific elements like the reviewer's name, star rating, review title, review date, and the review text.\n","\n","Here's the function to extract review data:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjIQsoNKmcos"},"outputs":[],"source":["# Grab Reviews name, description, date, stars, title from HTML\n","#This functiin retuens the data dictionary that will then be converted to\n","#pandas dataframe in order to upload to csv file.\n","def getReviews(html_data, prod_title, len_page):\n","\n","    # Create Empty list to Hold all data\n","    data_dicts = []\n","    # The below given code is to retrieve productname\n","    if prod_title:\n","        prod_name = prod_title\n","    else:\n","        prod_name = 'N/A'\n","\n","\n","    # Select all Reviews BOX html using css selector\n","    boxes = html_data.select('div[data-hook=\"review\"]')\n","\n","    # Iterate all Reviews BOX\n","    for box in boxes:\n","\n","        if prod_name:\n","            product_name = prod_name\n","\n","        try:\n","            name = box.select_one('[class=\"a-profile-name\"]').text.strip()\n","        except Exception as e:\n","            name = 'N/A'\n","\n","        try:\n","            stars = box.select_one('[data-hook=\"review-star-rating\"]').text.strip().split(' out')[0]\n","        except Exception as e:\n","            stars = 'N/A'\n","\n","        try:\n","            title = box.select_one('[data-hook=\"review-title\"]').text.strip()\n","        except Exception as e:\n","            title = 'N/A'\n","\n","        try:\n","            # Convert date str to dd/mm/yyy format\n","            datetime_str = box.select_one('[data-hook=\"review-date\"]').text.strip().split(' on ')[-1]\n","            date = datetime.strptime(datetime_str, '%B %d, %Y').strftime(\"%d/%m/%Y\")\n","        except Exception as e:\n","            date = 'N/A'\n","\n","        try:\n","            description = box.select_one('[data-hook=\"review-body\"]').text.strip()\n","        except Exception as e:\n","            description = 'N/A'\n","\n","        # create Dictionary with al review data\n","        data_dict = {\n","            'Product Name' : product_name,\n","            'Name' : name,\n","            'Stars' : stars,\n","            'Title' : title,\n","            'Date' : date,\n","            'Description' : description\n","        }\n","\n","        # Add Dictionary in master empty List\n","        data_dicts.append(data_dict)\n","\n","    return data_dicts"]},{"cell_type":"markdown","metadata":{"id":"if9ThtGpmhZD"},"source":["### Function to Determine Total Pages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3C_uMksQnMMa"},"outputs":[],"source":["def total_pages(url, page_allowance=300):\n","    try:\n","        # Send an HTTP GET request to the given URL with the specified headers\n","        response = requests.get(url, headers=headers)\n","        # Parse the HTML content of the response using BeautifulSoup with the 'html.parser' parser\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        total_pages = 1  # Initialize total pages to 1\n","\n","        # Find the element that contains the total review count\n","        total_reviews_element = soup.find('span', {'data-hook': 'total-review-count'})\n","        # Split the text of the total review count element to extract the count\n","        count = total_reviews_element.text.split()\n","\n","        reviews_per_page = 10  # Assume there are 10 reviews per page\n","\n","        # Extract the number part from the count\n","        str_num = count[0]\n","        # Remove any commas from the number string\n","        cleaned_str = str_num.replace(',', '')\n","\n","        if cleaned_str:\n","            # Convert the cleaned string to an integer\n","            num = int(cleaned_str)\n","            print(num)  # Print the total number of reviews\n","        else:\n","            print(\"Error: Invalid integer format\")  # Print an error message if the string is not a valid integer\n","\n","        total_reviews_count = num  # Store the total number of reviews\n","\n","        if total_reviews_count > 0:\n","            # Calculate the total number of pages based on the total reviews and reviews per page\n","            total_pages = (total_reviews_count // reviews_per_page) + 1\n","            # Limit the total pages to the specified page allowance\n","            if total_pages > page_allowance:\n","                total_pages = page_allowance\n","\n","        return total_pages  # Return the total number of pages\n","    except Exception as e:\n","        # Print an error message if an exception occurs during the process\n","        print(\"Error processing the response\")\n","\n"]},{"cell_type":"markdown","metadata":{"id":"c4p9KH3knM4J"},"source":["## Code Run\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YVl-118hnuYO"},"source":["### Determine the total number of pages and scrape the reviews."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sSK7hfXcwGI9"},"outputs":[],"source":["print('The url for which we are scraping reviews:',reviews_url)"]},{"cell_type":"markdown","metadata":{"id":"74s_dYEe_0XU"},"source":["Now let's evaluate how many pages of reviews are available for review. Note that we cap the pages to 300 to allow for timely execution. If you have more time, you can increase the page allowance below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HD2sQXUbro6T"},"outputs":[],"source":["# Extract number of pages\n","len_page = total_pages(reviews_url,page_allowance=300)\n","print(f\"Total Pages: {len_page}\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cdW_dqfQGTlm"},"outputs":[],"source":["len_page = 300"]},{"cell_type":"markdown","metadata":{"id":"vdik55QcAyH7"},"source":["Now let's extract all the html content for our reviews.\n","\n","**Important Note: This cell will take quite some time to run for products with higher reviews.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iaXM9CKGnVRq"},"outputs":[],"source":["# Get HTML data for all reviews\n","html_datas,prod_title = reviewsHtml(reviews_url, len_page)"]},{"cell_type":"markdown","metadata":{"id":"AImkaXdtBvaW"},"source":["Parse html data to extract review information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uOp_GEFtvbXc"},"outputs":[],"source":["# Empty List to Hold all reviews data\n","reviews = []\n","\n","# Iterate all Html page\n","for html_data in html_datas:\n","    # Grab review data\n","    review = getReviews(html_data,prod_title, len_page)\n","    # add review data in reviews empty list\n","    reviews += review\n","\n","# Create a dataframe with reviews Data\n","df_reviews = pd.DataFrame(reviews)\n","df_reviews.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"samXD1BLrBxs"},"outputs":[],"source":["df_reviews.shape"]},{"cell_type":"markdown","metadata":{"id":"bUFyNvdrnjpD"},"source":["### Saving the Data\n","\n","Save the reviews data to a CSV file.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":426,"status":"ok","timestamp":1720000972056,"user":{"displayName":"Clément Sampebgo","userId":"18280352312023246054"},"user_tz":0},"id":"Mh-U84tKnowN","outputId":"ff2bf325-8ba3-4f69-a0d1-e450cd014f93"},"outputs":[{"name":"stdout","output_type":"stream","text":["Data saved to bose_quietComfort_earbuds_II_amazon_product_reviews.csv\n"]}],"source":["filename = 'bose_quietComfort_earbuds_II_amazon_product_reviews.csv'\n","df_reviews.to_csv(filename, index=False)\n","print(f\"Data saved to {filename}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTlQfbTti4-0"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[{"file_id":"1b9ATkfJOPsAJEYkt6B79eAoP6tISf9j-","timestamp":1719956360506}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}